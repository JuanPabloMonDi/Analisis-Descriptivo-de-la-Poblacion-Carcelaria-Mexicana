---
title: "Métodos de agrupamiento (_Clustering_)"
subtitle: "Estadística Descriptiva Multivariada"
author: "Profesor: Camilo José Torres Jiménez"
institute: "Universidad Nacional de Colombia, Sede Bogotá"
format:
  revealjs: 
    chalkboard: false
    embed-resources: true
    self-contained-math: true
    html-math-method: katex
    smaller: true
    scrollable: true
    code-fold: true
    code-copy: true
    slide-number: true
    preview-links: auto
    link-external-newwindow: true
    #theme: dark
execute:
  echo: true
  error: false
  warning: false
  message: false
---


## Métodos de agrupamiento (_Clustering_)

* El objetivo de estos métodos es **descubrir patrones** en los
datos, en forma de **grupos bien diferenciados**, que tengan **individuos homogéneos**
en su interior.

* En las áreas de minería de datos, aprendizaje automático
y reconocimiento de patrones, estos métodos hacen parte de lo que se denominaría como **aprendizaje no supervisado** (_unsupervised learning_). 

* En la literatura francesa de análisis de datos (_d'analyse des données_) se los denomina **métodos de clasificación** (_techniques de classification_), mencionando que son **no supervisados** (_non supervisé_) cuando se necesita la aclaración. Por otro lado, **_Partitionnement de données_**, en francés, sería un término similar o equivalente a **_data clustering_** en inglés.

Ver Capítulo 7 de: Pardo, C. (2020). _**Estadística descriptiva multivariada**_. <https://repositorio.unal.edu.co/handle/unal/79914>


---

* En el sentido matemático, un algoritmo de agrupamiento busca una partición de un conjunto de elementos (los individuos) en subconjuntos (grupos). Esto es equivalente a asignarle una categoría a cada individuo (una etiqueta que indica el subconjunto o grupo al que pertenece), y por lo tanto es como **obtener una nueva variable cualitativa a partir de los datos**.

* Existen una gran cantidad de métodos de agrupamiento y diferentes tipos de ellos. En este módulo nos concentraremos, primero, en un **método que obtiene los grupos a partir de una sucesión de particiones (agrupamiento jerárquico)**, y luego, en un **método que obtiene los grupos a partir de la cantidad de grupos deseados, unos puntos iniciales y un criterio de parada (agrupamiento directo)**.

* Los algoritmos de agrupamiento suelen necesitar **medidas de similitud, disimilitud o distancia** entre individuos y entre grupos.

**Código a ejecutar para empezar (de clic en "Code" para desplegar el código)**:

```{r}
# cambia idioma de la consola de R a español:
Sys.setenv(LANG="es")
# usar 2 cifras significativas y tiende a evitar 
# notación científica (ver ayuda de función: `options`): 
options(digits = 2, scipen = 999) 
# cargar librerías: 
if(!require(FactoClass)){
  install.packages("FactoClass"); library(FactoClass)}
if(!require(Factoshiny)){
  install.packages("Factoshiny"); library(Factoshiny)}
if(!require(factoextra)){
  install.packages("factoextra"); library(factoextra)}
if(!require(plotly)){
  install.packages("plotly"); library(plotly)} # gráficos dinámicos
```

```{r}
#| echo: false
options(width = 100) 
if(!require(knitr)){
  install.packages("knitr"); library(knitr)} # tablas fijas
if(!require(DT)){
  install.packages("DT"); library(DT)} # tablas dinámicas
```


---

:::: {.columns}

::: {.column width="50%"}

Las **medidas de similitud** evalúan el grado de parecido o proximidad existente
entre dos elementos. Los valores más altos indican mayor parecido o proximidad
entre los elementos comparados.

:::

::: {.column width="50%"}

Las **medidas de disimilitud** ponen el énfasis en el grado de diferencia o lejanía
existente entre dos elementos. Los más altos indican mayor diferencia
o lejanía entre los elementos comparados.

:::

::::

Si la **medida de disimilitud** cumple la propiedad 1, se denomina **índice de distancia**; si verifica la propiedad 2, se llama **desviación**; si cumple la 1 y 2,
se llama **distancia** y si cumple las propiedades 1 y 3 (la propiedad 3 implica
la 2), se llama **ultra-métrica**:

1. $d(i,l) = 0$ implica que $i = l$.
2. $d(i,l) \leq d(i,k) + d(l,k)$ para todo $i, j, k$.
3. $d(i,l) \leq \max \{d(i,k) \, , \, d(l,k)\}$ para todo $i, j, k$.


---

### Agrupamiento jerárquico

[![](images/AgrupamientoJerarquico.png){fig-align="center" width="100%"}](images/AgrupamientoJerarquico.png)


---

#### Agrupamiento jerárquico aglomerativo

Un procedimiento de agrupamiento jerárquico aglomerativo procede de la
siguiente manera:

1. Seleccionar una medida de disimilitud (o distancia) entre individuos, y seleccionar un criterio de agregación o medida de disimilitud (o distancia) entre grupos.
1. Calcular las disimilitudes (o distancias) entre los $n$ individuos.
1. Unir los dos individuos menos disimiles, obteniendo $n - 1$ grupos.
1. Calcular las disimilitudes entre el nuevo grupo y los demás individuos (se pueden considerar grupos de un solo individuo).
1. Unir los dos grupos menos disimiles, obteniendo $n - 2$ grupos.
1. Continuar calculando las disimilitudes entre el nuevo grupo y los demás grupos, para luego, unir los dos grupos menos disimiles, hasta que obtenga un solo grupo con todos los individuos.

El proceso de uniones se representa por medio de un gráfico llamado **Dendrograma**.


---

Existen varias medidas de similitud, disimilitud y distancia entre individuos, dependiendo del tipo de variable y del contexto de aplicación. 

#### Algunas distancias entre individuos para variables continuas

**Distancia euclidiana**:

$$
\begin{aligned}
  d(i, l) = d(\vec{x}_i,\vec{x}_l) &= \sqrt{( x_{i1} - x_{l1} )^2 + \dots + ( x_{ip} - x_{lp} )^2}
\end{aligned}
$$

**Distancia Manhattan o _Cityblock_**:

$$
\begin{aligned}
  d(i, l) = d(\vec{x}_i,\vec{x}_l) &= | x_{i1} - x_{l1} | + \dots + | x_{ip} - x_{lp} |
\end{aligned}
$$

**Distancia del máximo o de Chebyschev**:

$$
\begin{aligned}
  d(i, l) = d(\vec{x}_i,\vec{x}_l) &= \max \{ |x_{i1} - x_{l1}| , \dots , |x_{ip} - x_{lp}| \}
\end{aligned}
$$


---

Para un procedimiento de aglomeración jerárquica se debe seleccionar
una similitud, disimilitud o distancia entre grupos, que se denomina
también **criterio de agregación**.

#### Dos criterios de agregación

**Enlace simple**: La distancia entre dos grupos $A$ y $B$ es igual a
la distancia de los dos individuos de diferente grupo más cercanos:
$$ d(A,B) = \min \{ d(i,l) : i \in A, l \in B  \} $$

Este criterio tiende a producir grupos
alargados (efecto de encadenamiento), que pueden incluir elementos
muy distintos en los extremos. 

**Enlace completo**: La distancia entre los dos
grupos $A$ y $B$ es igual a la  distancia entre los dos individuos de diferente grupo más alejados:
$$ d(A,B) = \max \{ d(i,l) : i \in A, l \in B  \} $$

El enlace completo tiende a producir grupos esféricos.


---

**Ilustremos con la distancia de Manhattan y el enlace completo:**

:::: {.columns}

::: {.column width="50%"}

```{r}
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5.5
#| fig-height: 5.5
a <- c(1,1); b <- c(2,2); c <- c(3,1)
d <- c(3,3); e <- c(3,4); f <- c(5,4)
X <- rbind(a,b,c,d,e,f)
s.label(X, clabel=1)
```

:::

::: {.column width="50%"}

Las distancias de Manhattan entre individuos son:

```{r}
D <- dist(X, "manhattan")
```

```{r}
#| echo: false
kable(as.matrix(D))
```


:::

::::


---

:::: {.columns}

::: {.column width="45%"}

Árbol o Dendrograma:

```{r}
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 5
hc <- hclust(D)
plot(hc, las=1, col="darkblue", main="", sub="", xlab="")
abline(h=0:7, col="gray90", lty=3)
```

:::

::: {.column width="50%"}

Pasos:

1. Se unen $d$ y $e$ a una distancia de 1
2. Se unen $a$ y $b$ a una distancia de 2
3. Se unen $ab$ y $c$ a una distancia de 2
4. Se unen $de$ y $f$ a una distancia de 3
5. Se unen $abc$ y $def$ a una distancia de 7

Ultra-métrica asociada al árbol:

```
  | a | b | c | d | e
  -------------------
b | 2 |       
c | 2 | 2      
d | 7 | 7 | 7    
e | 7 | 7 | 7 | 1  
f | 7 | 7 | 7 | 3 | 3
```

:::

::::


---

**Método de Ward**:

Para lograr grupos que tengan inercia "dentro-grupos" / "intra-grupos" mínima se debe utilizar
una distancia euclidiana y unir en cada paso del procedimiento los dos grupos
que menos aumenten la inercia dentro-grupos. Al **incremento de inercia dentro-grupos al unir dos grupos** lo llamaremos **distancia de Ward entre esos dos grupos** y la notaremos $W(,)$. Entonces, al tener tres grupos $A$, $B$ y $C$, es necesario calcular $W(A, B)$, $W(A, C)$ y $W(B, C)$ y el menor valor determinará los grupos a unir.

Sean $A$ y $B$ dos grupos o clases no vacías y disyuntas, y sean $p_A$, $p_B$ y $g_A$,
$g_B$, los pesos y centros de gravedad de los grupos $A$ y $B$ según
el caso.

$$
\begin{aligned}
  W(A, B) &= p_{_A} d^2(\mathbf{g}_{_{A}},\mathbf{g}_{_{AB}}) + p_{_B}  d^2(\mathbf{g}_{_{B}},\mathbf{g}_{_{AB}}) \\
  &= \frac{p_{_A}  p_{_B}}{p_{_A} + p_{_B}}d^{2}(\mathbf{g}_{_A} , \mathbf{g}_{_B}) 
\end{aligned}
$$

---

:::: {.columns}

::: {.column width="50%"}

En particular, para dos individuos $i$ y $l$ (considerados como grupos de un solo elemento), la distancia de Ward es:
$$
W(i,l) = \frac{p_i  p_l}{p_i + p_l}d^{2}(i,l)
$$

:::

::: {.column width="50%"}

Si los pesos para los dos individuos son iguales a $\tfrac{1}{n}$, entonces:
$$
W(i,l) = \frac{1}{2n}d^{2}(i,l)
$$

:::

::::

**Al formar el árbol:**

- Antes de empezar las uniones toda la inercia corresponde a **inercia entre-grupos** (cada individuo es un grupo).
- A medida que llevan a cabo las uniones, la **inercia entre-grupos** va pasando a **inercia dentro-grupos**
- Al terminar, toda es **inercia dentro-grupos** (todos los elementos conforman un grupo)


---

:::: {.columns}

::: {.column width="55%"}

Distancias de Ward entre cafés:

```{r}
#| code-fold: show
library(FactoClass); data(cafe)
acp <- dudi.pca(cafe, scannf=FALSE, nf=3)
F <- as.matrix(acp$li); n <- nrow(F)
Wcafe <- ( dist(F)^2 ) / (2*n)
round(Wcafe, 2)
```

:::

::: {.column width="45%"}

Árbol:

```{r}
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 5.5
hclCafe <- ward.cluster(dista=dist(F), h.clust=1) # función de FactoClass
plot(hclCafe, las=1, col="darkblue", main="", sub="", xlab="")
abline(h=seq(0, 7, 0.5), col="gray90", lty=3)
```

:::

::::


---

### Agrupamiento directo

En este caso, los métodos suelen requerir: la cantidad de grupos a obtener, valores iniciales para poder empezar y un criterio de parada para poder detenerse. Uno de los métodos
más sencillo y más conocido de agrupamiento directo (basado en particiones y basado en centroides) es el de **K-means**.

[![_Tomado de_: Jain, A. K. (2010). **Data clustering: 50 years
beyond K-means**. Pattern recognition letters, 31(8), 651-666.](images/Clustering.jpg){fig-align="center" height="300"}](images/Clustering.jpg)


---

**Método K-means**:

* Está relacionado con la geometría utilizada en los métodos factoriales porque recurre a la distancia Euclidiana entre individuos. 

* La distancia entre grupos se calcula como la distancia Euclidiana entre sus respectivos centros de gravedad. 

* El método **K-means** busca una partición en $K$ grupos que tenga **inercia dentro-grupos** mínima, es decir, lo que hace es minimizar las distancias euclidianas al cuadrado entre los individuos y el centro de gravedad del grupo al que pertenecen. 

* Además, el minimizar esta **inercia dentro-grupos** coincide con el buscar que los individuos de cada grupo sean lo más "parecidos" entre sí que se pueda (individuos homogéneos dentro de cada grupo). 

* Por otro lado, como **inercia (total)** = **inercia dentro-grupos** + **inercia entre-grupos**, entonces, también se está buscando que los individuos de grupos distintos sean lo más "diferentes" que se pueda (individuos heterogéneos entre grupos distintos).


---

**Algoritmo K-means (Algoritmo de Lloyd/Forgy)**

**ENTRADA**: Tabla de datos de variables cuantitativas y la cantidad de grupos a obtener $(K)$.

:::: {.columns}

::: {.column width="60%"}

* Pedir como entrada o seleccionar $K$ puntos ("centros de gravedad") iniciales.

**Repetir**:

* Asignar a cada individuo el grupo cuyo centro de gravedad sea el más cercano.
* Calcular los nuevos centros de gravedad de cada grupo.
    
**Hasta que**: los centros de gravedad no cambien.

:::

::: {.column width="40%"}

[![](images/kmeans-stepbystep.gif){fig-align="center" width="100%"}](images/kmeans-stepbystep.gif)

:::

::::

**SALIDA**: Conformación de los grupos (asociación individuo-grupo).


---

**Ventajas del K-means**:

* **Es muy rápido**. Usualmente converge en pocas iteraciones.
* **Poco exigente en recursos computacionales**. Por ejemplo, no requiere calcular distancias entre todos los individuos, basta con calcular las distancias de los individuos a los centros de gravedad de los grupos. Como $K$ suele ser mucho menor a $n$, la cantidad de distancias a calcular es menor.

**Aplicaciones**:

_**10 Interesting Use Cases for the K-Means Algorithm**_

<https://dzone.com/articles/10-interesting-use-cases-for-the-k-means-algorithm>


---

**Desventajas del K-means**:

* Se debe suministrar la cantidad de grupos a obtener. **De alguna manera se debe determinar el número de grupos más adecuado para el conjunto de datos**.

:::: {.columns}

::: {.column width="53%"}

* El resultado obtenido por el algoritmo depende de los puntos ("centros de gravedad") iniciales dados. **No hay garantía de que la inercia dentro-grupos alcanzada sea un mínimo global**, incluso podría estar bastante lejos de serlo.

* El método solamente es capaz de identificar adecuadamente grupos de individuos cuando estos se encuentran en regiones separables por "puntos equidistantes" con respecto a los centros de gravedad.

:::

::: {.column width="42%"}

[![](images/kmeans.svg){fig-align="center" width="100%"}](images/kmeans.svg)

:::

::::


---

### Combinación de métodos

* Los métodos de agrupación de Ward y de K-means son **compatibles**, ya que los dos **buscan la más baja inercia dentro-grupos**.

* Además, estos dos métodos **tienen ventajas y desventajas que se complementan**.

* La recomendación para combinar los métodos sería:

    1. Cuando el número de elementos a agrupar no es tan grande, y el equipo de cálculo lo permite, se realiza la **agrupación jerárquica aglomerativa con el método de Ward**. Si el número de elementos a clasificar es demasiado grande se puede utilizar el K-means para un pre-agrupamiento (miles de grupos), y luego realizar el agrupamiento jerárquico con los centros de gravedad del pre-agrupamiento.
    
    1. A partir del Dendrograma, es posible **escoger un corte del árbol** que nos daría la **cantidad de grupos potencialmente más adecuada**. 
    
    1. Luego, con los centros de gravedad dados por el corte en el árbol, utilizar el **K-means** para disminuir tanto como se pueda la inercia dentro-grupos, y así **mejorar o consolidar la agrupación** respectiva.


---

### Agrupamiento a partir de ejes factoriales

Los métodos factoriales se pueden utilizar para transformar los datos antes de realizar procedimientos de agrupación. Estos métodos pueden cumplir dos funciones: **reducir la dimensión de los datos** y **cuantificar las variables cualitativas**. Por ejemplo:

[![](images/clasVarNom.png){fig-align="center" height="420"}](images/clasVarNom.png)


---

### Caracterización automática

Luego de obtener los grupos de individuos, es natural el querer identificar las características que distinguen a cada grupo, de los demás grupos, o del conjunto completo de individuos.

Como al agrupar se obtiene una nueva variable cualitativa a partir de los datos, esta nueva variable se puede contrastar con las demás mediante un análisis bivariado. En particular, se pueden utilizar los **valores test**:

* Para una **variable cuantitativa**, los valores test son valores indicativos de que tan lejos, en desviaciones estándar, está la media de la variable cuantitativa para los individuos de un grupo, con respecto a la media general o global de la variable.

* En el caso de una **variable cualitativa**, un valor test nos indica que tan diferente es la frecuencia de una categoría dentro de un grupo, con respecto a su frecuencia global.

* Las variables cuantitativas o las categorías de las variables cualitativas con **un valor test en magnitud superior a un umbral se dice que caracterizan al grupo**.


---

### Estrategia completa de agrupamiento

La estrategia de agrupamiento que se ha propuesto, se resume en los siguientes pasos:

1. Realizar el análisis en ejes factoriales que corresponda.
2. Seleccionar el número de ejes factoriales para el agrupamiento.
3. Si el número de "individuos" es muy grande, realizar un K-means de pre-agrupamiento en miles de grupos.
4. Realizar el agrupamiento jerárquico con el método de Ward sobre los "individuos" o los grupos del paso anterior.
5. Decidir el número de grupos y cortar el árbol.
6. Realizar un K-means de consolidación, partiendo de los centros de gravedad de la partición obtenida al cortar el árbol.
7. Caracterizar los grupos.
8. Proyectar los grupos sobre los planos factoriales.


---

### Ejemplo de "juguete" (admitidos)

A continuación se procederá a realizar la agrupación de admitidos. Este análisis es complementario
al realizado en la sesión anterior. Las variables activas son género, edad,
estrato y origen, y las variables ilustrativas, los resultados del examen de admisión
(continuas) y la carrera.

```{r}
#| code-fold: show
library(FactoClass) # carga FactoClass
data(admi) # carga datos admi de FactoClass
Y <- admi[, c("gene","edad","estr","orig")] # activas
head(Y, 10)
```


---

Siguiendo la estrategia completa de agrupamiento propuesta:

:::: {.columns}

::: {.column width="48%"}

**1. El análisis en ejes factoriales que corresponde es un ACM.**

```{r}
#| eval: false
#| code-fold: show
# OPCIÓN 1:
# ejecuta de forma interactiva:
fc <- FactoClass(Y, dudi.acm, admi[, 1:7])
```

```{r}
#| echo: false
#| output: false
fc <- FactoClass(Y, dudi.acm, admi[, 1:7], scanFC=FALSE, nf=3, nfcl=6, k.clust=8)
```

```{r}
#| echo: false
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 2.7
barplot(as.vector(fc$dudi$eig), col="darkblue")
```

**2. Se seleccionan seis (6) ejes.**

:::

::: {.column width="4%"}

:::

::: {.column width="48%"}

**3. No se requiere un K-means de pre-agrupamiento.**

**4. Indices de la agrupación jerárquica:**

```{r}
#| echo: false
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 3.8
#| code-fold: show
barplot(rev(tail(fc$indices$Indice,15)), horiz=TRUE, col="darkblue")
```

**5. Se hace el corte para ocho (8) grupos.**

:::

::::


---

:::: {.columns}

::: {.column width="50%"}

**6. Cambios debidos al K-means:**

```{r}
#| code-fold: show
fc$clus.summ[, 1:4]
```

**7. Valores test para la caracterización:**

```{r}
#| eval: false
#| code-fold: show
# Valores test variables cuantitativas
fc$carac.cont
```

```{r}
#| eval: false
#| code-fold: show
# Valores test categorías de v. cualitativas 
fc$carac.cate
```

:::

::: {.column width="50%"}

**8. Gráfico del primer plano factorial:**

```{r}
#| fig-cap: ''
#| fig-align: 'center'
#| fig-width: 5
#| fig-height: 4.5
#| code-fold: show
plotFactoClass(fc, x=1, y=2, cex.col=0.5,
               roweti="", infaxes="no", cstar=0)
```

:::

::::


---

Si así lo prefiere, para seguir los pasos de la estrategia completa de agrupamiento, también puede optar por utilizar el paquete `Factoshiny`. Basta con: 

:::: {.columns}

::: {.column width="10%"}

:::

::: {.column width="80%"}

1. Hacer el análisis factorial que corresponda. Por ejemplo, para los datos de **Adjetivos × Colores** de la sesión 2, se requeriría un ACS (la función correspondiente sería `CAshiny()`):

```{r}
#| eval: false
#| code-fold: show
# OPCIÓN 2:
data("ColorAdjective") # requiere FactoClass cargado
res <- CAshiny(ColorAdjective) # requiere Factoshiny cargado
```

2. Seleccionar la opción "_**Perform clustering after leaving CA app?**_".

3. Indicar el número de ejes a conservar para el agrupamiento.

4. Dar clic en "_**Quit the app**_".

:::

::: {.column width="10%"}

:::

::::

Este último paso abre la aplicación web que tiene como título: "_**HCPC on the dataset**_", que es la encargada específicamente de la parte del agrupamiento, a partir de los resultados del análisis factorial.

